---
layout: post
title: '文本分类'
subtitle: '文本语义相似性匹配'
date: 2019-07-29
categories: 总结
author: yxz
cover: 'https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1564401601&di=65d4c18dbda45f33c82d458ffcead5e5&imgtype=jpg&er=1&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201708%2F03%2F20170803153553_HZRPv.jpeg'
tags: 文本语义
---

# 

[TOC]



## 1 概述

文本分类的传统的方法，需要人工提取选择和提取特征(人工构造)，同时难以处理词的顺序和上下文，对词义的表达能力较弱，对一词多义和一义多词不好处理。

![1564474037181]({{ "assets/blog_img/2019-07-29-文本分类.assets/1564474037181.png" | absolute_url }})

## 2 模型

### 2.1fastText：

![img](https://pic4.zhimg.com/80/v2-8e88ad8c42708993b0bf69fc0e44396b_hd.jpg)



和word2vec是同一个作者。**该模型首先把句子中所有的词向量平均（可认为是一种avg-pooling），然后接softmax层，非常简单直接。文章里还另外加了一些n-gram信息来捕捉局部序列信息。**

文章带来的思考是文本分类问题是有很大『线性成分』的，也就是说不必做得太复杂，即可捕获很多分类信息，因此有些文本分类任务简单的方法就能得到不错的效果。

### 2.2TextCNN:

![img](https://pic4.zhimg.com/80/v2-8786a5eb87f268c58195b2d8bf2fc36b_hd.jpg)

![img](https://pic4.zhimg.com/80/v2-5006f9d9b6581d1a1916ff9bdd164eb3_hd.jpg)

模型的具体说明：

embedding层：一般为word2vec，把高维稀疏的one-hot特征转变为低维稠密的连续特征。（采用方法MLP，SVM）

卷积层：经过有filter_size为2，3，4的一维卷积层，每个又有两个输出 channel

池化层：1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示，方便后面接全连接层。因为卷积层和池化层可以输入不同维度的向量，但一般全连接层不行。

softmax层：最后接全连接的 softmax 层，输出每个类别的概率



输入的词向量有静态（static）和非静态（nonstatic）两种。静态的方式就是词向量对应的参数在模型训练过程中不改变，即词向量本身是不可训练的参数。非静态就刚好相反，词向量里的参数是随着模型一起训练（fine-tunning）的，其实就是迁移学习了。



经过词向量表达的文本都是一维数据，所以用到的卷积都是一维的，和图像不同。因为另一个维度--词向量，如果卷积视野只看到词向量某一部分，则可能得不到一个完整的语义。同时这里设计了不同size的卷积来捕捉不同宽度的视野

### 2.3 TextRNN

![img](https://pic4.zhimg.com/80/v2-452e9f8e129ae7272ada3fc70babd753_hd.jpg)

TextRnn + attention

《NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO
ALIGN AND TRANSLATE》

其实也是attention机制的出处。虽然原文是用来做机器翻译的。

![img](https://pic2.zhimg.com/80/v2-bd818ef1fd00c4c40273f1d7a902e581_hd.jpg)


$$
u_i=tanh(W_sh_i+b_s)
$$

$$
\alpha_{i}=\frac { exp(u_i^{T}u_{s})}{\sum_{i}exp(u_i^Tu_s)}
$$

$$
v=\sum_i\alpha_ih_i
$$

Attention 首先是一个小型的网络构成，如下公式计算出$u_i$,然后接softmax来归一化，然后对应的隐含单元的hi的attention得分。这里的attention只与时刻t的隐含单元的$h_i$相关。

### 2.4RCNN

文章提出RNN是一个有偏模型，因为会更关注最近的词，而这个意义上CNN是无偏模型。但它的视野，即卷积的大小需要人工设置，不能自适应学习。文章提出一种结合，即RCNN，希望结合RNN和CNN的优点。

模型：

1 word embedding层

2双向RNN层，其输出的双向隐单元向量再和之前的word embedding concat起来形成一个词的表示

3.上面得到的每个词的表示，过一个全连接层

4 max-pooling层，使得不同长度的文章能得到同一长度的表示

5 softmax层

![preview](https://pic3.zhimg.com/v2-0c331bfe2189278a11f3ee33f89181c2_r.jpg)

Hierarchical Attention

Hierarchical Attention的出处。对于一个文本，作者认为文本由句子构成，句子由词构成，所以利用词向量+attention学习出句子向量，再由句子向量+attention学习出文本向量，然后再做分类。由于有两个层次的attention，所以称Hierarchical Attention。

![img](https://pic1.zhimg.com/80/v2-a8a537295f99ea4b30ad34b3e86d608c_hd.jpg)

文章中RNN cell是GRU

模型结构：

模型结构：

Word encoder的attention:

![img](https://pic4.zhimg.com/80/v2-e41b317edd2f00b5bedd3496fdaf2e93_hd.jpg)

Sentence encoder的attention类似

这是文本分类问题，不像翻译问题那样有多个输出，所以attention的构造都与St-1无关了，只与隐藏状态ht有关，用一个简单的nn来构造。